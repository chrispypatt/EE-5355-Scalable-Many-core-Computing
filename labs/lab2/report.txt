
QUESTION #1:

Briefly describe your implementation in one or two paragraphs as well as any 
difficulties you faced in developing and optimizing the kernel.

ANSWER:
My implememntation follows a similar style as the slides on 7-point stenciling
with coarsening and register tiling. Each thread essentially computes the output
along one column of the z-axis. To get reuse for this traveling up the grid, each
thread stores the first three elements of its column and then shifts those values 
down for the next column, reusing each inner value as a top, middle, and bottom 
value of Anext. 

I also implemented shared memory tiling. Each tile is a 30x30 
thread block so my shared memory was set as a 30x30 int metrix. Then each thread 
loads its center value into its threadIdx.x,threadIdx.y location of the matrix.
From there each thread can access its neighbor's values for i+1,i-1,j+1,j-1. This 
however, required  boundary checking so threads on the edge of a tile would loaded
a zero if it was on the edge of A0 or the neighbor value from A0 that the tile 
does not contain. This shared memory implementation requires all threads to store 
their middle value into the matrix before calculating Anext and for shifting z-values
only after all threads have completed their Anext calculation. To do this, I added 
__syncthreads() calls after each of those steps. 

Optimization was fairly straight forward but I did run into some difficulties along
the way. Specifically with regards to the boundaries in the Anext calculation. I 
based my boundaries off of the lecture slides which caused some out of bounds accesses.
This was specifically for loading j+1 and i+1 from shared memory. The check in the slides
was that (ty < dy) and (tx < dx) but that caused threads with tx or tx = 29 to 
try and load index 30 on the row or column axis from shared memory which is out of bounds. 
To fix this, the new boundary checks were changed to (ty < dy-1) and (tx < dx-1). 
Another issue whihc has given me sporatic failures was for loading j-1 values. 
The calculation originally tried was:

	((ty > 0)?  ds_A[ty-1][tx]: (j==0)?  0:A0(i,j-1,k))

which would occasionally load zeros when 1<=i<=27, j==1, k==1. Using cuda-gdb, the 
program showed the correct values would be loaded to the output calcualtion but then
after the kernel returned and we verified results, would be off only for the i,j and k
conditions above. I found that this issue was simply because I forgot to check if my i 
and j were within bounds, nx and ny, of the A0 array. This caused memory issues, resulting
in data being corrupted. To fix this, I added the check:

	if ((i < nx) && (j < ny)){

right after calculting the tread indicies at the beginning of the kernel.


QUESTION #2:

Consider a 100x100x50 7-point stencil that does not use any thread-coarsening or 
tiling optimizations. How much real computation does each work-item perform? How 
many global memory loads does each work-item perform? What is the ratio of 
computation to global memory loads for each work-item? (Consider as useful work 
only additions, subtractions, and multiplications that operate directly on the 
loaded data. Do not consider index computations.)

ANSWER:
With a work-item defined as the operation of one thread in our kernel execution,
considering the calculation of Anext as our real computation, each Anext 
calculation performs 7 additions and subtractions plus one multiplication. This
is a total of 8 real computations per calculation. With no tiling or thread-coarsening
optimizations, a tread will perform 7 global memory loads per z-axis element. We 
calculate nz-2 Anext elements per thread (z==1 and z==nz are not computed as they 
have no bottom or top neighbors respectively). So for this case, we have 48 calculations
with 8 computations and 7 global memory loads per calculation.

Thus, the ratio of computation to global memory loads is (48*8/7*48) = 1.14 loads per work item.


QUESTION #3:

Consider a 100x100x50 7-point stencil that uses thread-coarsening and joint 
tiling optimizations such as the one you implemented. How much real computation 
does each work-item perform? How many global memory loads does each work-item 
perform? What is the ratio of computation to global memory loads for each 
work-item? (Consider as useful work only additions, subtractions, and 
multiplications that operate directly on the loaded data. Do not consider index 
computations.)

ANSWER:
With optimization, the number of global loads for the z-axis per work-item is ny 
due to thread-coarsening where the first z calculation loads three elements followed
by 1 element for the top for 2<=z<=49. Then, because each thread stores its middle value
the shared memory matrix for each 2-D slice, the rest of the values are loaded from 
shared memory, not global memory. There are cases in a work group where threads on the
edge or corner of the tile will have to load between 1 to 2 more global values. These cases
will just be somewhere between question 2's ratio and the ratio I will calcualte for this problem
so I wont explicitly calcualate these two case's ratios.

The computations per work item stays the same at 8 computations per Anext element, and 48*8
compuations per work-item.

Thus the ratio between computation and global loads is (48*8)/50=7.68 loads per work item.





QUESTION #4:

Briefly comment on the difference in computation to global memory loads ratios 
for the two cases.

ANSWER:
We can see clearly that using shared memory tiling and thread coarsening allows us to 
get much more calculations from each global memory load. This is because of the way we
define work groups to cover layer of the z-axis at a time. This allows us to load every 
element we need for three layers at a time, with the middle layer in shared memory.
This allows for many threads to use the same memory values without the overhead of 
accessing global memory like we see in problem 2. Problem 2 is inefficent because we can 
see that every thread loads every computation value by itself but that load overlaps with 
the need of other thread which we take advantage of in our optimized version.


